name: Performance Testing

on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - 'api/**' 
      - 'perf/**'
      - '.github/workflows/perf.yml'
  workflow_dispatch:
    inputs:
      base_url:
        description: 'Base URL for performance testing'
        required: false
        default: ''

env:
  # Default to staging URL, can be overridden
  PERF_BASE_URL: ${{ github.event.inputs.base_url || vars.PERF_BASE_URL || 'https://staging.capsight.com' }}

jobs:
  perf:
    name: Performance Testing
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Wait for staging deployment
        if: github.event_name == 'pull_request'
        run: |
          echo "Waiting for staging environment to be ready..."
          sleep 30
          
          # Health check with retry
          max_attempts=10
          attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            echo "Health check attempt $attempt/$max_attempts..."
            
            if curl -f -s "${{ env.PERF_BASE_URL }}/api/health" > /dev/null; then
              echo "‚úÖ Staging environment is ready"
              break
            fi
            
            if [ $attempt -eq $max_attempts ]; then
              echo "‚ùå Staging environment not ready after $max_attempts attempts"
              exit 1
            fi
            
            sleep 10
            attempt=$((attempt + 1))
          done

      - name: Run k6 performance tests
        uses: grafana/k6-action@v0.3.1
        with:
          filename: perf/value.js
          flags: --out json=results.json --out junit=results.xml
        env:
          PERF_BASE_URL: ${{ env.PERF_BASE_URL }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ github.run_id }}
          path: |
            results.json
            results.xml
          retention-days: 30

      - name: Generate HTML report
        if: always()
        run: |
          # Install k6 HTML reporter
          npm install -g k6-html-reporter
          
          # Generate HTML report from JSON results
          k6-html-reporter --json-file results.json --output performance-report.html
          
          # Create summary for GitHub
          cat > performance-summary.md << 'EOF'
          ## üöÄ Performance Test Results
          
          **Target**: `${{ env.PERF_BASE_URL }}`
          **Commit**: `${{ github.sha }}`
          **Branch**: `${{ github.head_ref || github.ref_name }}`
          
          ### Key Metrics
          - **Load Pattern**: 1‚Üí50 VUs over 2 minutes
          - **Total Requests**: $(jq -r '.metrics.http_reqs.values.count // "N/A"' results.json)
          - **Request Rate**: $(jq -r '.metrics.http_req_rate.values.rate // "N/A"' results.json | xargs printf "%.2f") req/s
          - **Average Response Time**: $(jq -r '.metrics.http_req_duration.values.avg // "N/A"' results.json | xargs printf "%.2f") ms
          - **95th Percentile**: $(jq -r '.metrics.http_req_duration.values["p(95)"] // "N/A"' results.json | xargs printf "%.2f") ms
          - **Failure Rate**: $(jq -r '.metrics.http_req_failed.values.rate // "N/A"' results.json | xargs printf "%.4f")%
          
          ### Thresholds
          $(jq -r '.metrics | to_entries[] | select(.key | contains("threshold")) | "- **\(.key)**: \(.value.values.passes // 0) passes, \(.value.values.fails // 0) fails"' results.json || echo "Threshold details not available")
          
          üìä **[View Detailed HTML Report](./performance-report.html)**
          EOF

      - name: Upload HTML report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report-${{ github.run_id }}
          path: |
            performance-report.html
            performance-summary.md
          retention-days: 30

      - name: Comment PR with results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            // Read performance summary
            let summary = '';
            try {
              summary = fs.readFileSync('performance-summary.md', 'utf8');
            } catch (err) {
              summary = '‚ö†Ô∏è Could not generate performance summary';
            }
            
            // Check if thresholds passed
            let status = '‚úÖ';
            try {
              const results = JSON.parse(fs.readFileSync('results.json', 'utf8'));
              const thresholds = results.thresholds || {};
              const failed = Object.values(thresholds).some(t => t.ok === false);
              if (failed) status = '‚ùå';
            } catch (err) {
              status = '‚ö†Ô∏è';
            }
            
            const body = `${status} **Performance Test Results**\n\n${summary}\n\n---\n*Automated performance testing via k6*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Publish test results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Performance Tests
          path: results.xml
          reporter: java-junit
          fail-on-error: true

      - name: Check performance thresholds
        if: always()
        run: |
          # Parse results and check if any thresholds failed
          if [ -f results.json ]; then
            # Check for threshold failures
            failed_thresholds=$(jq -r '.thresholds // {} | to_entries[] | select(.value.ok == false) | .key' results.json)
            
            if [ ! -z "$failed_thresholds" ]; then
              echo "‚ùå Performance thresholds failed:"
              echo "$failed_thresholds" | while read -r threshold; do
                echo "  - $threshold"
              done
              echo ""
              echo "Performance requirements not met. Please optimize before merging."
              exit 1
            else
              echo "‚úÖ All performance thresholds passed!"
            fi
          else
            echo "‚ö†Ô∏è No results file found - test may have failed to run"
            exit 1
          fi
